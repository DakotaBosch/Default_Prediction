{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.impute import SimpleImputer\nfrom sklearn import metrics\nimport gc\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-24T06:31:58.488999Z","iopub.execute_input":"2022-08-24T06:31:58.489445Z","iopub.status.idle":"2022-08-24T06:31:58.499996Z","shell.execute_reply.started":"2022-08-24T06:31:58.489411Z","shell.execute_reply":"2022-08-24T06:31:58.498946Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"/kaggle/input/amex-parquet/test_data.parquet\n/kaggle/input/amex-parquet/train_data.parquet\n/kaggle/input/amex-default-prediction/sample_submission.csv\n/kaggle/input/amex-default-prediction/train_data.csv\n/kaggle/input/amex-default-prediction/test_data.csv\n/kaggle/input/amex-default-prediction/train_labels.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"df1=pd.read_parquet('/kaggle/input/amex-parquet/train_data.parquet')\ndf1.head()\ndf1.shape\ndf_label = pd.read_csv('/kaggle/input/amex-default-prediction/train_labels.csv')\ndf_label.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-24T06:31:58.524346Z","iopub.execute_input":"2022-08-24T06:31:58.525203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checking out data types\nprint(df1.dtypes.value_counts())\nlist(df1.select_dtypes(['object']).columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#visualizing columns\n\n#hist = df1.hist(bins=10, figsize = (40,200), layout=(-1,4) )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df1.shape[1])\n#inspecting NaN\nfor i in range(len(df1.columns)):\n    if (df1.iloc[:,i].isnull().sum()/len(df1) > 0.1):\n        print(df1.columns[i], round(df1.iloc[:,i].isnull().sum()/len(df1),2))\n\n#drop columns with high freq of NaN\ncolumns_to_drop = [column for column in df1.columns if df1[column].isnull().sum()/len(df1) >= 0.1]\ndf1.drop(columns_to_drop, axis=1, inplace=True)\nprint(df1.shape[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#using only most recent transaction from each customer\ndf1.shape\ndf1=df1.set_index(['customer_ID'])\ndf1=df1.ffill()\ndf1=df1.bfill()\ndf1=df1.reset_index()\n\ndf1=df1.groupby('customer_ID').tail(1)\ndf1=df1.set_index(['customer_ID'])\n\n#Drop date column since it is no longer useful\ndf1.drop(['S_2'],axis=1,inplace=True)\n\ndf1.shape\ndf1.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df1.shape)\n# Create correlation matrix\ncorr_matrix = df1.corr().abs()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Find features with correlation greater than 0.9\nto_drop = [column for column in upper.columns if any(upper[column] > 0.76)]\n\n# Drop features w/ high correl\ndf1.drop(to_drop, axis=1, inplace=True)\n\nprint(df1.shape)\ndf1.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Removing low variance columns in interest of ram\nfrom sklearn.feature_selection import VarianceThreshold\nfrom itertools import compress\n\ntemp = df1.drop(['D_63', 'D_64'], axis=1)\n\n# Initialize and fit the method\nvt = VarianceThreshold(threshold = float(0.1))\nvt.fit(temp)\n\n#columns with sufficient variance\nkeep = list(compress(temp.columns, vt.get_support()))\n\nkeep.append('D_63')\nkeep.append('D_64')\n\ndf1=df1[keep]\nlen(keep)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1.head()\n#df1 = df1.drop('target')\nkeep = df1.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#removing outliers\n##print(df1.shape)\n\n#df1 = df1[df1['R_6'] < df1['R_6'].quantile(0.97)]\n#print(df1['R_6'].max())\n#print(df1.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df1.iloc[:100000,7].value_counts()\n#print(df1.iloc[:,1].head())\n\n\n#What type of variable for dates\n#df1['S_2'] = pd.to_datetime(df1['S_2'])\n#df1['S_2'] = pd.to_numeric(df1['S_2'])\n\n#normalizing\n#df1['S_2'] = (df1['S_2']-df1['S_2'].min())/(df1['S_2'].max() - df1['S_2'].min())\n#print(df1['S_2'].head())\n\n#df1['S_2'] = pd.to_timedelta(df1['S_2'])\n#print(df1.iloc[:,1].dt.total_seconds())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Hot ones\ndf1 = pd.get_dummies(df1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Handling missing values\nmy_imputer = SimpleImputer()\ndf1.iloc[:,:] = my_imputer.fit_transform(df1.iloc[:,:])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df1.iloc[:, :].values.reshape(-1, len(df1.columns))\nY = df_label.iloc[:len(df1), 1].values.reshape(-1, 1)\nprint('half')\n# create object for the class\nlog = LogisticRegression()\nlog.fit(X, Y) \nY_pred = log.predict(X)\n\nprint(Y_pred, np.sum(Y_pred))\nprint(log.score(X, Y))\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = metrics.confusion_matrix(Y, Y_pred)\n\nplt.figure(figsize=(9,9))\nsns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'YlGnBu');\nplt.ylabel('Actual label');\nplt.xlabel('Predicted label');\nall_sample_title = 'Accuracy Score: {0}'.format(round(log.score(X, Y),3))\nplt.title(all_sample_title, size = 15);\n\nprint('Accuracy:',round(metrics.accuracy_score(Y, Y_pred),3))\nmetrics.roc_curve(Y, Y_pred)\nprint(metrics.roc_auc_score(Y, Y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#free up ram\ndel df1, df_label, cm\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#run prediction on test data\n\nprint(len(keep))\n#need to only load some columns due to ram limitations\ndf2=pd.read_parquet('/kaggle/input/amex-parquet/test_data.parquet', columns =keep)\nprint(df2.shape)\n\n#hot ones\ndf2 = pd.get_dummies(df2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Handling missing values\ndf2.iloc[:,:11] = my_imputer.fit_transform(df2.iloc[:,:11])\nprint('1')\ndf2.iloc[:,11:19] = my_imputer.fit_transform(df2.iloc[:,11:19])\nprint('2')\ndf2.iloc[:,19:30] = my_imputer.fit_transform(df2.iloc[:,19:30])\ndf2.iloc[:,30:] = my_imputer.fit_transform(df2.iloc[:,30:])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df2.iloc[:, :].values.reshape(-1, len(df2.columns))\nY_pred2 = log.predict(X)\n\nprint(Y_pred2, np.sum(Y_pred2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}